---
layout: default
title: "A Hybrid Compact Neural Architecture for Visual Place Recognition"
---

<center><h1>{{ page.title }}</h1></center>

<td>
	<center>
		<nobr>Marvin Chancán<sup>1,3</sup></nobr> &emsp;&emsp; <nobr>Luis Hernandez-Nunez<sup>2,3</sup></nobr> &emsp;&emsp; <nobr>Ajay Narendra<sup>4</sup></nobr> &emsp;&emsp; <nobr>Andrew B. Barron<sup>4</sup></nobr> &emsp;&emsp; <nobr>Michael Milford<sup>1</sup></nobr><br>
		<nobr><sup>1</sup>Queensland University of Technology</nobr> &emsp;&emsp; <nobr><sup>2</sup>Harvard University</nobr> &emsp;&emsp; <nobr><sup>3</sup>Universidad Nacional de Ingeniería</nobr> &emsp;&emsp; <nobr><sup>4</sup>Macquarie University</nobr><br>
		<br>
		<em>IEEE Robotics and Automation Letters (RA-L)</em><br>
		<br>
		<img style="vertical-align:middle" src="fn.png"  width="53%" height="inherit"/>		
	</center>
</td>

<br>
	
<td>
	<hr>
	<h3 style="margin-bottom:10px;">Abstract</h3>
	State-of-the-art algorithms for visual place recognition, and related visual navigation systems,
	can be broadly split into two categories: computer-science-oriented models including deep learning
	or image retrieval based techniques with minimal biological plausibility, and neuroscience-oriented
	dynamical networks that model temporal properties found in neural cells underlying spatial navigation
	in the brain. In this paper, we propose a new compact and high-performing place recognition hybrid
	model that bridges this divide for the first time. Our approach comprises two key components that
	incorporate neural models of these two categories: (1) FlyNet, a compact, sparse two-layer neural
	network inspired by brain architectures of fruit flies, Drosophila melanogaster, and (2) a one-dimensional
	continuous attractor neural network (CANN). The resulting FlyNet+CANN network combines the compact
	pattern recognition capabilities of our FlyNet model with the powerful temporal filtering capabilities
	of an equally compact CANN, replicating entirely in a hybrid neural implementation the functionality
	that yields high performance in algorithmic localization approaches like SeqSLAM. We evaluate our approach,
	and compare it to three state-of-the-art place recognition methods, on two benchmark real-world datasets
	with small viewpoint variations and extreme environmental changes; including day/night cycles where it
	achieves an AUC performance of 87% compared to 60% for Multi-Process Fusion, 46% for LoST-X and 1% for
	SeqSLAM, while being 6.5, 310, and 1.5 times faster respectively.

</td>

<td>
	<h3> arXiv: [<a href="https://arxiv.org/abs/1910.06840">arXiv</a>] &nbsp; &nbsp; &nbsp; Code: [<a href="https://github.com/mchancan/flynet">GitHub</a>] &nbsp; &nbsp; &nbsp; PDF: [<a href="MC_FlyNet_EA.pdf">PDF</a>] &nbsp; &nbsp; &nbsp; IEEE Xplore®: [<a href="https://doi.org/10.1109/LRA.2020.2967324">Letter</a>] </h3>
</td>

<br>
<br>

<h3 style="margin-bottom:0px;">Bibtex</h3>
<pre>
@article{chancan2020hybrid,
	author = {M. {Chanc\'an} and L. {Hernandez-Nunez} and A. {Narendra} and A. B. {Barron} and M. {Milford}},
	journal = {IEEE Robotics and Automation Letters},
	title = {A Hybrid Compact Neural Architecture for Visual Place Recognition},
	year = {2020},
	volume = {5},
	number = {2},
	pages = {993--1000},
	keywords = {Biomimetics;localization;visual-based navigation},
	doi = {10.1109/LRA.2020.2967324},
	ISSN = {2377-3774},
	month = {April}
}
</pre>
