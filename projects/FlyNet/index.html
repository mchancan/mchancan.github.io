---
layout: default
title: "A Compact Neural Architecture for Visual Place Recognition"
---

<center><h1>{{ page.title }}</h1></center>

<td>
	<center>
		<nobr>Marvin Chancán<sup>1,3</sup></nobr> &emsp;&emsp; <nobr>Luis Hernandez-Nunez<sup>2,3</sup></nobr> &emsp;&emsp; <nobr>Ajay Narendra<sup>4</sup></nobr> &emsp;&emsp; <nobr>Andrew B. Barron<sup>4</sup></nobr> &emsp;&emsp; <nobr>Michael Milford<sup>1</sup></nobr><br>
		<nobr><sup>1</sup>Queensland University of Technology</nobr> &emsp;&emsp; <nobr><sup>2</sup>Harvard University</nobr> &emsp;&emsp; <nobr><sup>3</sup>Universidad Nacional de Ingeniería</nobr> &emsp;&emsp; <nobr><sup>4</sup>Macquarie University</nobr><br>
		<br>
		arXiv:1915.xxxx (submitted to RA-L with ICRA 2020 option)<br>
		<br>
		<img style="vertical-align:middle" src="fn.png"  width="53%" height="inherit"/>		
	</center>
</td>

<br>
	
<td>
	<hr>
	<h3 style="margin-bottom:10px;">Abstract</h3>
	State-of-the-art algorithms for visual place recognition can be broadly
	split into two categories: computationally
	expensive deep-learning/image retrieval based techniques with
	minimal biological plausibility, and computationally cheap,
	biologically inspired models that yield poor performance in real
	world environments. In this paper we present a new compact
	and high-performing system that bridges this divide for the first
	time. Our approach comprises two key components: FlyNet, a
	compact, sparse two-layer neural network inspired by fruit fly
	brain architectures, and a one-dimensional continuous attractor
	neural network (CANN). Our FlyNet+CANN network combines
	the compact pattern recognition capabilities of the FlyNet
	model with the powerful temporal filtering capabilities of an
	equally compact CANN, replicating entirely in a neural network
	implementation the functionality that yields high performance
	in algorithmic localization approaches like SeqSLAM. We
	evaluate our approach and compare it to three state-of-the-art methods
	on two benchmark real-world datasets with small
	viewpoint changes and extreme appearance variations including
	different times of day (afternoon to night) where it achieves an
	AUC performance of 87%, compared to 60% for Multi-Process
	Fusion, 46% for LoST-X and 1% for SeqSLAM, while being
	6.5, 310, and 1.5 times faster respectively.
</td>

<td>
	<h3> Manuscript: [<a href="2019_FlyNet.pdf">PDF</a>] &nbsp; &nbsp; &nbsp; Code: [<a href="https://github.com/mchancan/flynet">GitHub</a>] &nbsp; &nbsp; &nbsp; arXiv: [<a href="https://arxiv.org/abs/1910.xxxx">arXiv</a>] </h3>
</td>

<br>
<br>

<h3 style="margin-bottom:0px;">Bibtex</h3>
<pre>
@article{
	FlyNetMC19,
	author = {Chanc\'an, Marvin and Hernandez-Nunez, Luis and Narendra, Ajay and Barron, Andrew B. and Milford, Michael},
	title = {A Compact Neural Architecture for Visual Place Recognition},
	volume = {abs/1915.xxxx},
	year = {2019},
	url = {https://arxiv.org/abs/1915.xxxx},
	archivePrefix = {arXiv},
	eprint = {1915.xxxx},
    	primaryClass = {cs.CV}
}
</pre>
